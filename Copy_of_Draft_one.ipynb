{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e85UKYfmhfKW"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "metadata": {
        "id": "X4FSF36phhRQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfbaad6d-a814-4428-e30b-fe05de3b07b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LayerNormalization"
      ],
      "metadata": {
        "id": "GMT4OvSghk6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "eDHZAz4IhnhW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c750f45-1bbf-41b7-c4ac-dcd116625066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "metadata": {
        "id": "-V4K7xoghtwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ac3eaef-9f9a-4100-d3c5-4c5cdfa73fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 15585658829643583175\n",
            "xla_global_id: -1\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "metadata": {
        "id": "fzSil9Z3ikqz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5171532-1cc1-467b-8072-a0b3c840eab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-069mg8pq\n",
            "  Running command git clone --filter=blob:none --quiet https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-069mg8pq\n",
            "  warning: redirecting to https://github.com/keras-team/keras-contrib.git/\n",
            "  Resolved https://www.github.com/keras-team/keras-contrib.git to commit 3fc5ef709e061416f4bc8a92ca3750c824b5d2b0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-contrib==2.0.8) (2.15.0)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-py3-none-any.whl size=101055 sha256=36556224bd6179403f73db1edc188950b522a3edc81abaab3899cdcf8e653a23\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-se_2l_q1/wheels/74/d5/f7/0245af7ac33d5b0c2e095688649916e4bf9a8d6b3362a849f5\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade keras\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install -U tensorflow\n",
        "!python -m pip show tensorflow\n",
        "!pip install --upgrade tensorflow\n",
        "!python -m pip show tensorflow\n",
        "!pip install tensorflow-addons\n",
        "!pip install -q tensorflow-addons\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install utils"
      ],
      "metadata": {
        "id": "S2u1-oMrixrU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e69240a-e442-4a2c-fe3a-0a83a7122ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.11.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.11.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Collecting h5py>=3.10.0 (from tensorflow)\n",
            "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.2)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
            "Installing collected packages: ml-dtypes, h5py, tensorboard, tensorflow\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h5py-3.11.0 ml-dtypes-0.3.2 tensorboard-2.16.2 tensorflow-2.16.1\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
            "Name: tensorflow\n",
            "Version: 2.16.1\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, requests, setuptools, six, tensorboard, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
            "Required-by: dopamine-rl, tf_keras\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
            "Name: tensorflow\n",
            "Version: 2.16.1\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, requests, setuptools, six, tensorboard, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
            "Required-by: dopamine-rl, tf_keras\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
            "Collecting utils\n",
            "  Downloading utils-1.0.2.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: utils\n",
            "  Building wheel for utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for utils: filename=utils-1.0.2-py2.py3-none-any.whl size=13906 sha256=1dcfa26e32db27af85d8342bec7ed04c2973e23c6925c74487ba48ac71d742a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/39/f5/9d0ca31dba85773ececf0a7f5469f18810e1c8a8ed9da28ca7\n",
            "Successfully built utils\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyedflib"
      ],
      "metadata": {
        "id": "Ycx7DRTpi3N9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a702c084-5251-4827-b6fe-2c94aacabfce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyedflib\n",
            "  Downloading pyEDFlib-0.1.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from pyedflib) (1.25.2)\n",
            "Installing collected packages: pyedflib\n",
            "Successfully installed pyedflib-0.1.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install padasip"
      ],
      "metadata": {
        "id": "ZsoK0J10jkJ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2405ad34-585e-43cf-8790-d3383f481024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting padasip\n",
            "  Downloading padasip-1.2.2.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from padasip) (1.25.2)\n",
            "Building wheels for collected packages: padasip\n",
            "  Building wheel for padasip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for padasip: filename=padasip-1.2.2-py3-none-any.whl size=51650 sha256=5265067d4dc5c0d8f49a2282da92b4a0ed8d409378313ecac86cb7bb968adff5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/bb/e7/b57f417dbad7d85dbcb177f2c185543ca34a1b36541f115e95\n",
            "Successfully built padasip\n",
            "Installing collected packages: padasip\n",
            "Successfully installed padasip-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fileNames = []\n",
        "fileName_str = []\n",
        "\n",
        "for i in range(10,11,1):\n",
        "  name_str = ''\n",
        "  if(i<10):\n",
        "    name_str = 'b00'\n",
        "  else:\n",
        "    name_str = 'b0'\n",
        "  fileNames.append(name_str+ str(i)+'.edf')\n",
        "  fileName_str.append(name_str+ str(i))"
      ],
      "metadata": {
        "id": "pyI0H5YWj_bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import butter, filtfilt, find_peaks"
      ],
      "metadata": {
        "id": "ItDZpjvckCqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyedflib\n",
        "import numpy as np\n",
        "import pywt  # Importing the Wavelet Transformation module\n",
        "from scipy.signal import cheby2, filtfilt, butter, find_peaks\n",
        "\n",
        "class DataUtils:\n",
        "    def __init__(self, fileNames) -> None:\n",
        "        super().__init__()\n",
        "        self.fileNames = fileNames\n",
        "\n",
        "    def readData(self, sigNum, path=\"/content/drive/MyDrive/drive-download-20230428T081947Z-001/\"):\n",
        "        file_name = path + self.fileNames[sigNum]\n",
        "        f = pyedflib.EdfReader(file_name)\n",
        "        n = f.signals_in_file\n",
        "        signal_labels = f.getSignalLabels()\n",
        "        print(\"Reading file:\", file_name)\n",
        "        print(\"Different columns:\", signal_labels)\n",
        "        print(\"Total number of samples:\", f.getNSamples())\n",
        "\n",
        "        scg = np.zeros((n, f.getNSamples()[0]))\n",
        "        abdECG = np.zeros((n, f.getNSamples()[0]))\n",
        "\n",
        "        # Read signals from all available channels\n",
        "        for i in range(n):\n",
        "            if i == 3:  # SCG channel\n",
        "                scg[i, :] = f.readSignal(i)\n",
        "                scg[i, :] = self.apply_wavelet_transform(scg[i, :])\n",
        "            elif i == 0:  # ECG channel\n",
        "                abdECG[i, :] = f.readSignal(i)\n",
        "\n",
        "        return scg, abdECG\n",
        "\n",
        "    def apply_wavelet_transform(self, signal):\n",
        "        # Wavelet decomposition\n",
        "        coeffs = pywt.wavedec(signal, 'db4', level=4)  # Wavelet decomposition with 'db4' wavelet and 4 decomposition levels\n",
        "        reconstructed_signal = pywt.waverec(coeffs, 'db4')  # Reconstruction of the signal\n",
        "        return reconstructed_signal\n",
        "\n",
        "    def cheby2_bandpass_filter(self, data, lowcut, highcut, fs, order=3, rs=40):\n",
        "        nyq = 0.5 * fs\n",
        "        low = lowcut / nyq\n",
        "        high = highcut / nyq\n",
        "        b, a = cheby2(order, rs, [low, high], btype='band')\n",
        "        y = filtfilt(b, a, data)\n",
        "        return y\n",
        "\n",
        "    def qrs_detection(self, ecg, fs):\n",
        "        qrs_indices = []\n",
        "\n",
        "        for level_ecg in ecg:\n",
        "            qrs_level = self.pan_tompkins_on_level(level_ecg, fs)\n",
        "            qrs_indices.extend(qrs_level)\n",
        "\n",
        "        qrs_indices = sorted(list(set(qrs_indices)))\n",
        "\n",
        "        return qrs_indices\n",
        "\n",
        "    def pan_tompkins_on_level(self, ecg_level, fs):\n",
        "        nyquist = 0.5 * fs\n",
        "        low_cutoff = 5\n",
        "        high_cutoff = 15\n",
        "        b, a = butter(1, [low_cutoff/nyquist, high_cutoff/nyquist], btype='band')\n",
        "        ecg_filt = filtfilt(b, a, ecg_level)\n",
        "\n",
        "        b = np.array([1, 0, -1])\n",
        "        ecg_diff = np.convolve(ecg_filt, b, mode='same')\n",
        "        ecg_sq = ecg_diff ** 2\n",
        "\n",
        "        ma_len = int(0.08 * fs)\n",
        "        ecg_ma = np.convolve(ecg_sq, np.ones(ma_len)/ma_len, mode='same')\n",
        "\n",
        "        qrs_idx, _ = find_peaks(ecg_ma, distance=int(0.2 * fs), height=0.2 * np.max(ecg_ma))\n",
        "\n",
        "        return qrs_idx\n",
        "\n",
        "\n",
        "fileNames = ['b001.edf','b002.edf','b003.edf','b004.edf','b005.edf','b006.edf','b007.edf','b008.edf','b009.edf','b0010.edf','b0011.edf','b0012.edf','b0013.edf','b0014.edf','b0015.edf','b0016.edf','b0017.edf','b0018.edf','b0019.edf','b0020.edf']  # Replace with your list of file names\n",
        "data_utils = DataUtils(fileNames)\n",
        "\n",
        "# Read data from the first file\n",
        "scg, abdECG = data_utils.readData(0)\n",
        "\n",
        "# Perform QRS detection on the ECG signals after wavelet transformation\n",
        "qrs_indices = data_utils.qrs_detection(abdECG, fs=150)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WA8uTzIOuG3",
        "outputId": "bdebc7ff-4158-4606-cd10-c6b30a7a5e28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading file: /content/drive/MyDrive/drive-download-20230428T081947Z-001/b001.edf\n",
            "Different columns: ['I', 'II', 'RESP', 'SCG']\n",
            "Total number of samples: [1365000 1365000 1365000 1365000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyedflib\n",
        "import numpy as np\n",
        "import pywt\n",
        "from scipy.signal import butter, filtfilt, find_peaks\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class DataUtils:\n",
        "    def __init__(self, fileNames) -> None:\n",
        "        super().__init__()\n",
        "        self.fileNames = fileNames\n",
        "\n",
        "    def readData(self, sigNum, path=\"/content/drive/MyDrive/drive-download-20230428T081947Z-001/\"):\n",
        "        file_name = path + '/' + self.fileNames[sigNum]\n",
        "        f = pyedflib.EdfReader(file_name)\n",
        "        n = f.signals_in_file\n",
        "        signal_labels = f.getSignalLabels()\n",
        "\n",
        "        scg = np.zeros((n, f.getNSamples()[0]))\n",
        "        abdECG = np.zeros((n, f.getNSamples()[0]))\n",
        "\n",
        "        # Read signals from all available channels\n",
        "        for i in range(n):\n",
        "            if i == 3:  # SCG channel\n",
        "                scg[i, :] = f.readSignal(i)\n",
        "                scg[i, :] = self.apply_wavelet_transform(scg[i, :])  # Apply wavelet transform\n",
        "            elif i == 0:  # ECG channel\n",
        "                abdECG[i, :] = f.readSignal(i)\n",
        "\n",
        "        return scg, abdECG\n",
        "\n",
        "    def apply_wavelet_transform(self, signal):\n",
        "        coeffs = pywt.wavedec(signal, 'db4', level=4)  # Wavelet decomposition\n",
        "        reconstructed_signal = pywt.waverec(coeffs, 'db4')  # Inverse wavelet transform\n",
        "        return reconstructed_signal\n",
        "\n",
        "\n",
        "\n",
        "fileNames = ['b001.edf', 'b002.edf', 'b003.edf']  # Replace with your list of file names\n",
        "dataUtils = DataUtils(fileNames)\n",
        "\n",
        "# Read SCG signals and apply wavelet transform\n",
        "scg_signals, _ = dataUtils.readData(0)\n",
        "print(\"Original SCG signal shape:\", scg_signals.shape)\n"
      ],
      "metadata": {
        "id": "YwW64Pd-JMwE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f41c34a4-fcf7-4995-f337-5104228eb5fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original SCG signal shape: (4, 1365000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Added 2 Discriminators"
      ],
      "metadata": {
        "id": "-zd46ZYiSwia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* line 103, 104 self.image shape c and d\n",
        "* line 124 - 140 discriminator c and d\n",
        "* line 151, 152 modified generators\n",
        "* line 167, 168 added fake_c and d\n",
        "* line 286 and 287 added batches C and D\n",
        "* line 299-302 added imgs_c, imgs_d\n",
        "* line 310, 311 fake_c and d\n",
        "* line 315, 316 added reconstr_c and d\n",
        "* line 324 added img_c and d for g_loss\n",
        "* line 326-332 added dC and dD loss real and fake\n",
        "* line 335 modified d_loss\n",
        "* line 376-384 added fake_c and d, reconstr_c and d\n",
        "* line 394 added imgs_C, fake_D, reconstr_C, imgs_D, fake_C, reconstr_D to gen_imgs\n"
      ],
      "metadata": {
        "id": "wav7bNamS6dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class TrainUtils:\n",
        "    def __init__(self, fileNames) -> None:\n",
        "        super().__init__()\n",
        "        self.fileNames = fileNames\n",
        "        self.dataUtils = DataUtils(fileNames)\n",
        "\n",
        "    def prepareData(self, delay=5, path=\"/content/drive/MyDrive/semester project data set/drive-download-20230428T081947Z-001/b010.edf\"):\n",
        "        scgAll, ecg, fs = self.dataUtils.readData(0, path)\n",
        "        scgAll = scgAll[range(1), :]\n",
        "        delayNum = scgAll.shape[0]\n",
        "        ecgAll = self.dataUtils.createDelayRepetition(ecg, delayNum, delay)\n",
        "\n",
        "        for i in range(1, len(self.fileNames)):\n",
        "            scg, ecg = self.dataUtils.readData(i, path)\n",
        "            scg = scg[range(1), :]\n",
        "            ecgDelayed = self.dataUtils.createDelayRepetition(ecg, 1, delay)\n",
        "            scgAll = np.append(scgAll, scg, axis=1)\n",
        "            ecgAll = np.append(ecgAll, ecgDelayed, axis=1)\n",
        "\n",
        "        print(\"ECG all merged shape:: \", ecgAll.shape)\n",
        "\n",
        "        original_scg = scgAll\n",
        "        original_ecg = ecgAll\n",
        "        scgWindows, ecgWindows = self.dataUtils.windowingSig(scgAll, ecgAll, windowSize=1000)\n",
        "        return scgWindows, ecgWindows\n",
        "\n",
        "    def trainTestSplit(self, sig, label, trainPercent, shuffle=True):\n",
        "        print(\"Splitting into train and test:: \")\n",
        "        X_train, X_test, y_train, y_test = train_test_split(sig, label, train_size=trainPercent, shuffle=False)\n",
        "        X_train = np.array(X_train)\n",
        "        X_test = np.array(X_test)\n",
        "        y_train = np.array(y_train)\n",
        "        y_test = np.array(y_test)\n",
        "        return X_train, X_test, y_train, y_test\n"
      ],
      "metadata": {
        "id": "TciXapesJgLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class WaveletCycleGAN:\n",
        "    def __init__(self, row, col):\n",
        "        self.img_rows = row\n",
        "        self.img_cols = col\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols)\n",
        "        self.img_shape_b = (self.img_rows, 1)\n",
        "        self.dataset_name = 'ECG2FECG'\n",
        "\n",
        "        # Loss weights\n",
        "        self.lambda_cycle = 4.0  # Cycle-consistency loss\n",
        "        self.lambda_id = 0.01 * self.lambda_cycle  # Identity loss\n",
        "\n",
        "        # Build and compile the discriminators\n",
        "        self.d_A1 = self.build_discriminator(self.img_shape)\n",
        "        self.d_A2 = self.build_discriminator(self.img_shape_b)\n",
        "        self.d_B1 = self.build_discriminator(self.img_shape_b)\n",
        "        self.d_B2 = self.build_discriminator(self.img_shape)\n",
        "\n",
        "        # Build the generators\n",
        "        self.g_AB = self.build_generator(self.img_shape)\n",
        "        self.g_BA = self.build_generator(self.img_shape_b)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = Adam()\n",
        "\n",
        "        # Compile discriminators\n",
        "        self.d_A1.compile(loss='MSE', optimizer=optimizer, metrics=['accuracy'])\n",
        "        self.d_A2.compile(loss='MSE', optimizer=optimizer, metrics=['accuracy'])\n",
        "        self.d_B1.compile(loss='MSE', optimizer=optimizer, metrics=['accuracy'])\n",
        "        self.d_B2.compile(loss='MSE', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "        # Combined model\n",
        "        img_A = Input(shape=self.img_shape)\n",
        "        img_B = Input(shape=self.img_shape_b)\n",
        "        fake_B = self.g_AB(img_A)\n",
        "        fake_A = self.g_BA(img_B)\n",
        "        reconstr_A = self.g_BA(fake_B)\n",
        "        reconstr_B = self.g_AB(fake_A)\n",
        "        img_A_id = self.g_BA(img_A)\n",
        "        img_B_id = self.g_AB(img_B)\n",
        "        self.combined = Model(inputs=[img_A, img_B],\n",
        "                              outputs=[self.d_A1(fake_A), self.d_B1(fake_B),\n",
        "                                       self.d_A2(fake_B), self.d_B2(fake_A),\n",
        "                                       fake_B, fake_A,\n",
        "                                       reconstr_A, reconstr_B])\n",
        "        self.combined.compile(loss=['huber_loss', 'huber_loss', 'huber_loss', 'huber_loss',\n",
        "                                     'huber_loss', 'huber_loss', 'huber_loss', 'huber_loss'],\n",
        "                              loss_weights=[1, 1, self.lambda_cycle, self.lambda_cycle,\n",
        "                                            self.lambda_id, self.lambda_id], optimizer=optimizer)\n",
        "\n",
        "    def build_generator(self, img_shape):\n",
        "        def conv1DWithSINE(layer_input, filters, f_size=60):\n",
        "            d = Conv1D(filters, kernel_size=f_size, padding='same', activation='LeakyReLU')(layer_input)\n",
        "            d = tfa.layers.InstanceNormalization()(d)\n",
        "            d = BatchNormalization()(d)\n",
        "            return d\n",
        "\n",
        "        def multiply(x):\n",
        "            mask, image = x\n",
        "            return image * K.clip(mask, 0.8, 1)\n",
        "\n",
        "        input = Input(shape=img_shape)\n",
        "        value = conv1DWithSINE(input, 30, f_size=60)\n",
        "        att = TransformerBlock(embed_dim=input.shape[1], num_heads=2)(value)\n",
        "        att = Normalization(axis=1)(att)\n",
        "        remainedInput = Lambda(multiply)([att, value])\n",
        "        output_img = conv1DWithSINE(remainedInput, 17, f_size=240)\n",
        "        output_img = conv1DWithSINE(output_img, 13, f_size=240)\n",
        "        output_img = conv1DWithSINE(output_img, 13, f_size=240)\n",
        "        output_img = conv1DWithSINE(output_img, 1, f_size=1)\n",
        "        return Model(input, output_img)\n",
        "\n",
        "    def build_discriminator(self, img_shape):\n",
        "        def d_layer(layer_input, filters, f_size=33, normalization=True):\n",
        "            d = Conv1D(filters, kernel_size=f_size, padding='same', activation='LeakyReLU')(layer_input)\n",
        "            if normalization:\n",
        "                d = tfa.layers.InstanceNormalization()(d)\n",
        "                d = BatchNormalization()(d)\n",
        "            return d\n",
        "\n",
        "        img = Input(shape=img_shape)\n",
        "        d1 = d_layer(img, self.df)\n",
        "        d2 = d_layer(d1, 13)\n",
        "        d3 = d_layer(d2, 13)\n",
        "        d4 = d_layer(d3, 13)\n",
        "        d5 = d_layer(d4, 13)\n",
        "        d6 = d_layer(d5, 13)\n",
        "        validity = d_layer(d6, 1)\n",
        "\n",
        "        return Model(img, validity)\n"
      ],
      "metadata": {
        "id": "g3N36_T-V6Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class WaveletCycleGAN:\n",
        "    def __init__(self, row, col):\n",
        "        self.img_rows = row\n",
        "        self.img_cols = col\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols)\n",
        "        self.img_shape_b = (self.img_rows, 1)\n",
        "        self.dataset_name = 'ECG2FECG'\n",
        "\n",
        "        # Loss weights\n",
        "        self.lambda_cycle = 4.0  # Cycle-consistency loss\n",
        "        self.lambda_id = 0.01 * self.lambda_cycle  # Identity loss\n",
        "\n",
        "        # Build and compile the discriminators\n",
        "        self.d_A1 = self.build_discriminator(self.img_shape)\n",
        "        self.d_A2 = self.build_discriminator(self.img_shape_b)\n",
        "        self.d_B1 = self.build_discriminator(self.img_shape_b)\n",
        "        self.d_B2 = self.build_discriminator(self.img_shape)\n",
        "\n",
        "        # Build the generators\n",
        "        self.g_AB = self.build_generator(self.img_shape)\n",
        "        self.g_BA = self.build_generator(self.img_shape_b)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = Adam()\n",
        "\n",
        "        # Compile discriminators\n",
        "        self.d_A1.compile(loss='MSE', optimizer=optimizer, metrics=['accuracy'])\n",
        "        self.d_A2.compile(loss='MSE', optimizer=optimizer, metrics=['accuracy'])\n",
        "        self.d_B1.compile(loss='MSE', optimizer=optimizer, metrics=['accuracy'])\n",
        "        self.d_B2.compile(loss='MSE', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "        # Combined model\n",
        "        img_A = Input(shape=self.img_shape)\n",
        "        img_B = Input(shape=self.img_shape_b)\n",
        "        fake_B = self.g_AB(img_A)\n",
        "        fake_A = self.g_BA(img_B)\n",
        "        reconstr_A = self.g_BA(fake_B)\n",
        "        reconstr_B = self.g_AB(fake_A)\n",
        "        img_A_id = self.g_BA(img_A)\n",
        "        img_B_id = self.g_AB(img_B)\n",
        "        self.combined = Model(inputs=[img_A, img_B],\n",
        "                              outputs=[self.d_A1(fake_A), self.d_B1(fake_B),\n",
        "                                       self.d_A2(fake_B), self.d_B2(fake_A),\n",
        "                                       fake_B, fake_A,\n",
        "                                       reconstr_A, reconstr_B])\n",
        "        self.combined.compile(loss=['huber_loss', 'huber_loss', 'huber_loss', 'huber_loss',\n",
        "                                     'huber_loss', 'huber_loss', 'huber_loss', 'huber_loss'],\n",
        "                              loss_weights=[1, 1, self.lambda_cycle, self.lambda_cycle,\n",
        "                                            self.lambda_id, self.lambda_id], optimizer=optimizer)\n",
        "\n",
        "    def build_generator(self, img_shape):\n",
        "        def conv1DWithSINE(layer_input, filters, f_size=60):\n",
        "            d = Conv1D(filters, kernel_size=f_size, padding='same', activation='relu')(layer_input)\n",
        "            d = tfa.layers.InstanceNormalization()(d)\n",
        "            d = BatchNormalization()(d)\n",
        "            return d\n",
        "\n",
        "        def multiply(x):\n",
        "            mask, image = x\n",
        "            return image * K.clip(mask, 0.8, 1)\n",
        "\n",
        "        input = Input(shape=img_shape)\n",
        "        value = conv1DWithSINE(input, 30, f_size=60)\n",
        "        att = TransformerBlock(embed_dim=input.shape[1], num_heads=2)(value)\n",
        "        att = Normalization(axis=1)(att)\n",
        "        remainedInput = Lambda(multiply)([att, value])\n",
        "        output_img = conv1DWithSINE(remainedInput, 17, f_size=240)\n",
        "        output_img = conv1DWithSINE(output_img, 13, f_size=240)\n",
        "        output_img = conv1DWithSINE(output_img, 13, f_size=240)\n",
        "        output_img = conv1DWithSINE(output_img, 1, f_size=1)\n",
        "        return Model(input, output_img)\n",
        "\n",
        "    def build_discriminator(self, img_shape):\n",
        "        def d_layer(layer_input, filters, f_size=33, normalization=True):\n",
        "            d = Conv1D(filters, kernel_size=f_size, padding='same', activation='relu')(layer_input)\n",
        "            if normalization:\n",
        "                d = tfa.layers.InstanceNormalization()(d)\n",
        "                d = BatchNormalization()(d)\n",
        "            return d\n",
        "\n",
        "        img = Input(shape=img_shape)\n",
        "        d1 = d_layer(img, self.df)\n",
        "        d2 = d_layer(d1, 13)\n",
        "        d3 = d_layer(d2, 13)\n",
        "        d4 = d_layer(d3, 13)\n",
        "        d5 = d_layer(d4, 13)\n",
        "        d6 = d_layer(d5, 13)\n",
        "        validity = d_layer(d6, 1)\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "\n",
        "class TrainUtils:\n",
        "    def __init__(self):\n",
        "\n",
        "        pass\n",
        "\n",
        "\n",
        "from unittest import TestCase\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class TestWaveletCycleGAN(TestCase):\n",
        "\n",
        "    def __init__(self, methodName: str = ...) -> None:\n",
        "        super().__init__(methodName)\n",
        "        self.trainUtils = TrainUtils()\n",
        "\n",
        "    def test_trainSignal(self, path=\"/content/drive/MyDrive/drive-download-20230428T081947Z-001/\"):\n",
        "        # Adjust data preparation and loading steps to match the requirements of WaveletCycleGAN\n",
        "        scgWindows, ecgWindows = self.trainUtils.prepareData(delay=2, path=path)\n",
        "        # Adjust data splitting and reshaping steps accordingly\n",
        "        X_train, X_test, Y_train, y_test = self.trainUtils.trainTestSplit(scgWindows, ecgWindows, 0.75)\n",
        "        X_train = np.reshape(X_train, [-1, X_train.shape[1], X_train.shape[2]])\n",
        "        Y_train = np.reshape(Y_train, [-1, Y_train.shape[1], Y_train.shape[2]])\n",
        "\n",
        "        # Instantiate WaveletCycleGAN instead of CycleGAN\n",
        "        waveletCycleGAN = WaveletCycleGAN(X_train.shape[1], X_train.shape[2])\n",
        "        waveletCycleGAN.train(x_train=X_train, y_train=Y_train, epochs=1)\n",
        "\n",
        "        # Add assertions to validate the correctness of outputs\n",
        "        # Add more comprehensive testing scenarios as needed\n",
        "\n",
        "    def divide_test_train(self, path=\"/content/drive/MyDrive/drive-download-20230428T081947Z-001/\"):\n",
        "        # Adjust data preparation and loading steps to match the requirements of WaveletCycleGAN\n",
        "        scgWindows, ecgWindows = self.trainUtils.prepareData(delay=2, path=path)\n",
        "        # Adjust data splitting steps accordingly\n",
        "        X_train, X_test, Y_train, y_test = self.trainUtils.trainTestSplit(scgWindows, ecgWindows, 0.75)\n",
        "        return X_train, X_test, Y_train, y_tes"
      ],
      "metadata": {
        "id": "iPRRDaI8WIAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unittest import TestCase\n",
        "\n",
        "\n",
        "# Adjust imports based on the classes and modules used in WaveletCycleGAN\n",
        "# from path.to.WaveletCycleGAN import WaveletCycleGAN\n",
        "# from path.to.TrainUtils import TrainUtils\n",
        "\n",
        "class TestWaveletCycleGAN(TestCase):\n",
        "\n",
        "    def __init__(self, methodName: str = ...) -> None:\n",
        "        super().__init__(methodName)\n",
        "        self.trainUtils = TrainUtils()\n",
        "\n",
        "    def test_trainSignal(self, path=\"/content/drive/MyDrive/drive-download-20230428T081947Z-001/\"):\n",
        "        # Adjust data preparation and loading steps to match the requirements of WaveletCycleGAN\n",
        "        scgWindows, ecgWindows = self.trainUtils.prepareData(delay=2, path=path)\n",
        "        # Adjust data splitting and reshaping steps accordingly\n",
        "        X_train, X_test, Y_train, y_test = self.trainUtils.trainTestSplit(scgWindows, ecgWindows, 0.75)\n",
        "        X_train = np.reshape(X_train, [-1, X_train.shape[1], X_train.shape[2]])\n",
        "        Y_train = np.reshape(Y_train, [-1, Y_train.shape[1], Y_train.shape[2]])\n",
        "\n",
        "        # Instantiate WaveletCycleGAN instead of CycleGAN\n",
        "        waveletCycleGAN = WaveletCycleGAN(X_train.shape[1], X_train.shape[2])\n",
        "        waveletCycleGAN.train(x_train=X_train, y_train=Y_train, epochs=1)\n",
        "\n",
        "        # Add assertions to validate the correctness of outputs\n",
        "        # Add more comprehensive testing scenarios as needed\n",
        "\n",
        "    def divide_test_train(self, path=\"/content/drive/MyDrive/drive-download-20230428T081947Z-001/\"):\n",
        "        # Adjust data preparation and loading steps to match the requirements of WaveletCycleGAN\n",
        "        scgWindows, ecgWindows = self.trainUtils.prepareData(delay=2, path=path)\n",
        "        # Adjust data splitting steps accordingly\n",
        "        X_train, X_test, Y_train, y_test = self.trainUtils.trainTestSplit(scgWindows, ecgWindows, 0.75)\n",
        "        return X_train, X_test, Y_train, y_test\n"
      ],
      "metadata": {
        "id": "osUz5HtdPfJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class TransformerBlock:\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        # Define other necessary layers and operations for the Transformer block\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        # Implement the forward pass logic for the Transformer block\n",
        "        x = inputs\n",
        "        return x\n",
        "\n",
        "\n",
        "class WaveletCycleGAN:\n",
        "    def __init__(self, row, col):\n",
        "        self.img_rows = row\n",
        "        self.img_cols = col\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols)\n",
        "        self.img_shape_b = (self.img_rows, 1)\n",
        "        self.dataset_name = 'ECG2FECG'\n",
        "        self.df = 64  # Define the df attribute here\n",
        "\n",
        "        # Loss weights\n",
        "        self.lambda_cycle = 4.0  # Cycle-consistency loss\n",
        "        self.lambda_id = 0.01 * self.lambda_cycle  # Identity loss\n",
        "\n",
        "        # Build and compile the discriminators\n",
        "        self.d_A1 = self.build_discriminator(self.img_shape)\n",
        "        self.d_A2 = self.build_discriminator(self.img_shape_b)\n",
        "        self.d_B1 = self.build_discriminator(self.img_shape_b)\n",
        "        self.d_B2 = self.build_discriminator(self.img_shape)\n",
        "\n",
        "        # Build the generators\n",
        "        self.g_AB = self.build_generator(self.img_shape)\n",
        "        self.g_BA = self.build_generator(self.img_shape_b)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = Adam()\n",
        "\n",
        "        # Compile discriminators\n",
        "        self.d_A1.compile(loss='MSE', optimizer=optimizer, metrics=['accuracy'])\n",
        "        self.d_A2.compile(loss='MSE', optimizer=optimizer, metrics=['accuracy'])\n",
        "        self.d_B1.compile(loss='MSE', optimizer=optimizer, metrics=['accuracy'])\n",
        "        self.d_B2.compile(loss='MSE', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "        # Combined model\n",
        "        img_A = Input(shape=self.img_shape)\n",
        "        img_B = Input(shape=self.img_shape_b)\n",
        "        fake_B = self.g_AB(img_A)\n",
        "        fake_A = self.g_BA(img_B)\n",
        "        reconstr_A = self.g_BA(fake_B)\n",
        "        reconstr_B = self.g_AB(fake_A)\n",
        "        img_A_id = self.g_BA(img_A)\n",
        "        img_B_id = self.g_AB(img_B)\n",
        "        self.combined = Model(inputs=[img_A, img_B],\n",
        "                              outputs=[self.d_A1(fake_A), self.d_B1(fake_B),\n",
        "                                       self.d_A2(fake_B), self.d_B2(fake_A),\n",
        "                                       fake_B, fake_A,\n",
        "                                       reconstr_A, reconstr_B])\n",
        "        self.combined.compile(loss=['huber_loss', 'huber_loss', 'huber_loss', 'huber_loss',\n",
        "                                     'huber_loss', 'huber_loss', 'huber_loss', 'huber_loss'],\n",
        "                              loss_weights=[1, 1, self.lambda_cycle, self.lambda_cycle,\n",
        "                                            self.lambda_id, self.lambda_id], optimizer=optimizer)\n",
        "\n",
        "    def build_generator(self, img_shape):\n",
        "        def conv1DWithSINE(layer_input, filters, f_size=60):\n",
        "            d = Conv1D(filters, kernel_size=f_size, padding='same', activation='LeakyReLU')(layer_input)\n",
        "            d = tfa.layers.InstanceNormalization()(d)\n",
        "            d = BatchNormalization()(d)\n",
        "            return d\n",
        "\n",
        "        def multiply(x):\n",
        "            mask, image = x\n",
        "            return image * K.clip(mask, 0.8, 1)\n",
        "\n",
        "        input = Input(shape=img_shape)\n",
        "        value = conv1DWithSINE(input, 30, f_size=60)\n",
        "        att = TransformerBlock(embed_dim=input.shape[1], num_heads=2)(value)\n",
        "        att = LayerNormalization(axis=1)(att)\n",
        "        remainedInput = Lambda(multiply)([att, value])\n",
        "        output_img = conv1DWithSINE(remainedInput, 17, f_size=240)\n",
        "        output_img = conv1DWithSINE(output_img, 13, f_size=240)\n",
        "        output_img = conv1DWithSINE(output_img, 13, f_size=240)\n",
        "        output_img = conv1DWithSINE(output_img, 1, f_size=1)\n",
        "        return Model(input, output_img)\n",
        "\n",
        "    def build_discriminator(self, img_shape):\n",
        "        def d_layer(layer_input, filters, f_size=33, normalization=True):\n",
        "            d = Conv1D(filters, kernel_size=f_size, padding='same', activation='LeakyReLU')(layer_input)\n",
        "            if normalization:\n",
        "                d = tfa.layers.InstanceNormalization()(d)\n",
        "                d = BatchNormalization()(d)\n",
        "            return d\n",
        "\n",
        "        img = Input(shape=img_shape)\n",
        "        d1 = d_layer(img, self.df, f_size=64)  # Use self.df with a filter size of 64\n",
        "        d2 = d_layer(d1, 13)\n",
        "        d3 = d_layer(d2, 13)\n",
        "        d4 = d_layer(d3, 13)\n",
        "        d5 = d_layer(d4, 13)\n",
        "        d6 = d_layer(d5, 13)\n",
        "        validity = d_layer(d6, 1)\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def train(self, x_train, y_train, epochs):\n",
        "        # Define the training loop and update the generator and discriminator models\n",
        "        for epoch in range(epochs):\n",
        "            # Implement the training logic here\n",
        "            # You can use the combined model self.combined to train the generator and discriminator models\n",
        "            # Update the weights of the models based on the training data x_train and y_train\n",
        "\n",
        "\n",
        "            for batch_x, batch_y in zip(x_train, y_train):\n",
        "                g_loss, d_loss = self.combined.train_on_batch([batch_x, batch_y], [])\n",
        "                print(f\"Epoch {epoch+1}, Generator Loss: {g_loss}, Discriminator Loss: {d_loss}\")\n",
        "\n",
        "class TrainUtils:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def prepareData(self, delay, path):\n",
        "\n",
        "\n",
        "\n",
        "        # Simulating data for demonstration purposes\n",
        "        scgWindows = np.random.rand(100, 1000, 1)\n",
        "        ecgWindows = np.random.rand(100, 1000, 1)\n",
        "\n",
        "        return scgWindows, ecgWindows\n",
        "\n",
        "    def trainTestSplit(self, scgWindows, ecgWindows, split_ratio):\n",
        "\n",
        "        # Split data into training and testing sets\n",
        "        num_samples = len(scgWindows)\n",
        "        split_idx = int(num_samples * split_ratio)\n",
        "\n",
        "        X_train = scgWindows[:split_idx]\n",
        "        Y_train = ecgWindows[:split_idx]\n",
        "        X_test = scgWindows[split_idx:]\n",
        "        Y_test = ecgWindows[split_idx:]\n",
        "\n",
        "        return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "from unittest import TestCase\n",
        "\n",
        "class TestWaveletCycleGAN(TestCase):\n",
        "    def __init__(self, methodName: str = ...) -> None:\n",
        "        super().__init__(methodName)\n",
        "        self.trainUtils = TrainUtils()\n",
        "\n",
        "    def test_trainSignal(self, path=\"/content/drive/MyDrive/drive-download-20230428T081947Z-001/\"):\n",
        "        scgWindows, ecgWindows = self.trainUtils.prepareData(delay=2, path=path)\n",
        "        X_train, X_test, Y_train, Y_test = self.trainUtils.trainTestSplit(scgWindows, ecgWindows, 0.75)\n",
        "        X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "        Y_train = np.reshape(Y_train, (Y_train.shape[0], Y_train.shape[1], 1))\n",
        "\n",
        "        waveletCycleGAN = WaveletCycleGAN(X_train.shape[1], X_train.shape[2])\n",
        "        waveletCycleGAN.train(x_train=X_train, y_train=Y_train, epochs=1)\n",
        "\n",
        "        # Add assertions to validate the correctness of outputs\n",
        "        # Add more comprehensive testing scenarios as needed\n",
        "\n",
        "    def divide_test_train(self, path=\"/content/drive/MyDrive/drive-download-20230428T081947Z-001/\"):\n",
        "        scgWindows, ecgWindows = self.trainUtils.prepareData(delay=2, path=path)\n",
        "        X_train, X_test, Y_train, Y_test = self.trainUtils.trainTestSplit(scgWindows, ecgWindows, 0.75)\n",
        "        return X_train, X_test, Y_train, Y_test"
      ],
      "metadata": {
        "id": "xdW9TAf2th0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTxPDmi3f6I7",
        "outputId": "efaebe81-b550-4957-c7dc-be5db91d9e4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.29)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.3)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import tensorflow as tf\n",
        "from keras.optimizers import Adam\n",
        "from  keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "pcZz3wfsgCac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
        "    n_filters = trial.suggest_categorical('n_filters', [16, 32, 64, 128])\n",
        "\n",
        "    model = WaveletCycleGAN(n_filters=n_filters)\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "    scgWindows, ecgWindows = TrainUtils().prepareData(delay=5)\n",
        "    X_train, X_test, Y_train, Y_test = TrainUtils().trainTestSplit(scgWindows, ecgWindows, 0.75)\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) #We have to adjust the value of the patience based on the loss encounterd\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, Y_train,\n",
        "        epochs=500,\n",
        "        validation_data=(X_test, Y_test),\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    val_loss = np.min(history.history['val_loss'])\n",
        "\n",
        "    return val_loss\n"
      ],
      "metadata": {
        "id": "sqojSDwrgDwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=1000)\n",
        "print('Best trial:', study.best_trial.params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "3rwk7tWjgLar",
        "outputId": "f9bd87f4-187c-4889-dd1d-7c4c033f41c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-25 13:46:56,699] A new study created in memory with name: no-name-54615aa8-f01b-426e-88f0-e1d157a48baa\n",
            "[W 2024-04-25 13:46:56,702] Trial 0 failed with parameters: {'learning_rate': 0.00012067176476415494, 'n_filters': 16} because of the following error: TypeError(\"WaveletCycleGAN.__init__() got an unexpected keyword argument 'n_filters'\").\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-21-ed0a05a7a9c7>\", line 5, in objective\n",
            "    model = WaveletCycleGAN(n_filters=n_filters)\n",
            "TypeError: WaveletCycleGAN.__init__() got an unexpected keyword argument 'n_filters'\n",
            "[W 2024-04-25 13:46:56,706] Trial 0 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "WaveletCycleGAN.__init__() got an unexpected keyword argument 'n_filters'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-ec1cbdde9a6c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'minimize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best trial:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    449\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \"\"\"\n\u001b[0;32m--> 451\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     ):\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-ed0a05a7a9c7>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mn_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'n_filters'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWaveletCycleGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_filters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: WaveletCycleGAN.__init__() got an unexpected keyword argument 'n_filters'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = TestWaveletCycleGAN(\"test_trainSignal\")"
      ],
      "metadata": {
        "id": "Dr5p4NaGRIHJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}